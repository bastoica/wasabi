# Helper Scripts

This README outlines the functionality and provides sample usage examples for each essential helper script. These scripts are designed to help reproduce the key results from our evaluation of WASABI. We encourate users to check out our [paper](https://bastoica.github.io/files/papers/2024_sosp_wasabi.pdf) for details.

### § `run.py`

The `run.py` script automates WASABI's multiphase pipeline for benchmarking a set of target applications, currently Hadoop, HBase, Hive, Cassandra, and Elasticsearch. It facilitates cloning repositories, preparing code by replacing configuration files and rewriting source code, running fault injection tests, and executing bug oracles to analyze test and build reports.

Usage:
```
python3 run.py --phase <phase> --benchmark <benchmark>
```

Arguments:
* `--phase`: Specifies the phase of the pipeline,
  * `setup`: Clone the repository and checkout a specific version for the benchmark.
  * `prep`: Prepare the code by replacing configuration files and rewriting source code.
  * `bug-triggering`: Run fault injection tests on the benchmark.
  * `bug-oracles`: Run bug oracles to analyze test and build reports.
  * `all`: Execute all WASABI's phases, in sequence.

### § `run_benchmarks.py`

The `run_benchmarks.py` script automates WASABI's phase of running fault injection and identifying retry bugs for a target application. It performs several tasks: cleaning up local package directories to prevent build conflicts, building WASABI and the target application, running the test suite with fault injection, and saving the output logs for analysis.

Usage:
```
python3 run_benchmarks.py --benchmark <benchmark>
```

Arguments:
* `--benchmark`: Specifies the benchmark application to build and test. Current choices include `hadoop`, `hbase`, `hive`, `cassandra`, and `elasticsearch`.

### § `bug_oracles.py`

The `bug_oracles.py` analyzes log files generated during the testing of a target application. It processes both build and test logs to identify and categorize "HOW" and "WHEN" type retry bugs.

Usage:
```
python3 bug_oracles.py <logs_root_dir> --benchmark <benchmark>
```

Arguments:
* `<logs_root_dir>`: The root directory where the build and test logs are saved.
* `--benchmark`: Specifies the benchmark application for which to analyze logs. Current choices include `hadoop`, `hbase`, `hive`, `cassandra`, and `elasticsearch`.

### § `source_rewriter.py`

The `source_rewriter.py` script automates WASABI's phase of modifying of test files to adjust retry bounds and timeout values in large-scale applications. Operating in two modes, bounds-rewriting and timeout-rewriting, the script either increases the retry limits or extends the timeout durations in test methods, based on a given specification.

* `--mode`: Specifies the operation mode of the script. Choices are:
   * `bounds-rewriting`: Modifies retry bounds in Java code to a higher value.
   * `timeout-rewriting`: Adjusts timeout annotations and wait calls in Java test methods to a higher value.
* `<config_file>`: Path to the configuration file listing the changes to be made. The format depends on the mode:
  * For bounds-rewriting, it should contain variable names, assigned values, assignment methods, and test class names.
  * For timeout-rewriting, it should list the test classes and test methods that require timeout adjustments.
* `<target_root_dir>`: The root directory of the target application where the script searches for test files to modify.

### § `display_bug_results.py`

The `display_bug_results.py` analyzes bug reports generated by the test suite of a target application during fault injection, aggregates them based on bug types and the application name, compares them to the ground truth dataset from our [paper](https://bastoica.github.io/files/papers/2024_sosp_wasabi.pdf), and prints summary tables similar to Table 3 (see "4. Evaluation", page 9).

### § `generate_aspect.py`

The `generate_aspect.py` script automates the creation of AspectJ code for injecting exceptions into specific methods of a target application. It reads a specification file that details which exceptions to inject and where to inject them. The specification file is tailored for the given target application and contains the methods implementing retry along with their source code locations, the retry-triggering exceptions being handled, and the methods being retried along with their source code locations. Using this information, the script generates an AspectJ aspect that can be woven into the target application to simulate retry-triggering exceptions at specific program points that should trigger retries when such exceptions occur.

Usage:
```
python3 generate_aspect.py --spec-file <spec_file> --aspect-file <aspect_file>
```

Arguments:
* `--spec-file`: Path to the input specification file containing exception injection details. This file should be in CSV format (though it use custom delimiters, `!!!`) and includes entries that specify the methods implementing retry, their source code locations along with the retry-triggering exceptions being handled, and the methods being retried along with their source code locations. Each line has the following format:
```
[Retry Enclosing Method Location]!!![Enclosing Method]!!![Retried Method]!!![Retried Method Location]!!![Exception Class]
```
Check out the main [README](https://github.com/bastoica/wasabi/blob/master/wasabi-testing/README.md) file for more details.
* `--aspect-file`: Path to the output file to save the generated AspectJ aspect.

### § `test_plan_generator.py`

The `test_plan_generator.py` script automates the generation of test plans by matching retry locations with test cases in a target application. The script implements a heuristic that tries to ensure that each test is uniquely matched to a retry location, so no test is exercising two distinct retriable methods. The intuition is that once an exception is injected for a retried method at location `A`, another retried method executing at a later location `B` might not execute since the test could crash or hang.

Usage:
```
python3 test-plan-generation.py --retry_locations_input <retry_locations_input_file> \
                                --test_retry_pairs_input <test_retry_pairs_input_file> \
                                --path_to_configs <config_files_path>
```

Arguments:
* `--retry_locations_input`: Path to the input file containing details about retry locations.
* `--test_retry_pairs_input`: Path to the input file mapping tests to retry locations. Each line should contain a test name and an injection location (retried method), separated by a comma.
* `--path_to_configs`: Path where the generated configuration files should be saved.

### § `wasabi_coverage.py`

The `wasabi_coverage.py` script analyzes log files generated by a target application woven (instrumented) with WASABI, to determine code coverage statistics. Specifically, the script scans through test output logs to identify which methods have been instrumented and which have actually had exceptions injected during testing.
